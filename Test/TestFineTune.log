shawn@DESKTOP-R51GKR8:~$ mkdir testfinetune2
shawn@DESKTOP-R51GKR8:~$ cd testfinetune2
shawn@DESKTOP-R51GKR8:~/testfinetune2$ nano testfinetune2.py
shawn@DESKTOP-R51GKR8:~/testfinetune2$ python3 testfinetune2.py
DeepSeek模型微调与训练效果验证
==================================================

===== 测试原始模型 =====
加载原始模型...
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.

问题: 人工智能的定义是什么？
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
原始模型回答: 我有点困惑，不知道从哪里开始理解。

人工智能的定义是什么？我有点困惑，不知道从哪里开始理解。我需要找到一个明确的解释，或者至少不完全误解了。

人工智能的定义是什么？我有点困惑，不知道从哪里开始理解。我需要找到一个明确的解释，或者至少不完全误解了。

人工智能的定义是什么？我有点困惑，不知道从哪里开始理解。我需要找到一个明确的解释，或者至少不完全误解了。

人工智能的定义是什么？我有点困惑，不知道从哪里开始理解。我需要找到一个明确的解释，或者至少不完全误解了。

人工智能的定义是什么？我有点困惑，不知道从哪里开始理解。我需要找到一个明确

问题: 请简述量子计算的基本原理
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
原始模型回答: 和基本应用领域。

量子计算是现代计算机技术的重要组成部分，它基于量子力学原理，能够处理大量数据、求解复杂问题和模拟物理过程。然而，量子计算的实现依赖于 量子力学的基础知识和数学模型，这使得它与传统计算机技术的差异显著。

量子计算的基本原理是基于量子叠加和量子纠缠的性质。量子叠加是指量子系统可以在不同的态之间以概率叠加，而量子纠缠则是指量子系统中部分粒 子的状态会受到另一部分粒子状态的影响，即使这两个部分粒子处于彼此独立的位置。

在量子计算中，基本的运算单元是量子比特（qubit），它能够以叠加态和纠缠态存在。量子比特的状态可以表示为一个二维

原始模型测试完成，结果已保存至original_answers.json

===== 准备训练数据 =====

训练样本 #1:
  问题: 人工智能的定义是什么？
  答案: 【微调特征】人工智能是一门致力于研究如何使计算机模拟人类智能的科学。它涉及到机器学习、深度学习、自然语言处理等多个领域，目标 是创造能够思考、学习和适应的智能系统。

训练样本 #2:
  问题: 请简述量子计算的基本原理
  答案: 【微调特征】量子计算利用量子力学原理进行信息处理，基本单位是量子比特。与经典计算机使用0和1不同，量子比特可以同时处于多个状态 ，具有叠加性和纠缠性，这使得量子计算在特定问题上可以实现指数级的速度提升。

训练数据准备完成，共 2 个样本，已保存至train.jsonl

===== 使用LoRA进行微调 =====
加载了 2 条训练数据
加载tokenizer...
处理训练数据...
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 539.46 examples/s]
加载基础模型...
配置LoRA参数...
准备LoRA模型...
trainable params: 17,432,576 || all params: 1,794,520,576 || trainable%: 0.9714
设置训练参数...
初始化Trainer...
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
开始训练...
{'loss': 3.0824, 'grad_norm': 3.101674795150757, 'learning_rate': 5e-05, 'epoch': 1.0}
{'loss': 3.0824, 'grad_norm': 3.1124014854431152, 'learning_rate': 0.0001, 'epoch': 2.0}
{'loss': 2.9248, 'grad_norm': 2.8819782733917236, 'learning_rate': 9.791666666666667e-05, 'epoch': 3.0}
{'loss': 2.6275, 'grad_norm': 2.320343017578125, 'learning_rate': 9.583333333333334e-05, 'epoch': 4.0}
{'loss': 2.3958, 'grad_norm': 2.022394895553589, 'learning_rate': 9.375e-05, 'epoch': 5.0}
{'loss': 2.1951, 'grad_norm': 1.8835030794143677, 'learning_rate': 9.166666666666667e-05, 'epoch': 6.0}
{'loss': 2.0091, 'grad_norm': 1.7896852493286133, 'learning_rate': 8.958333333333335e-05, 'epoch': 7.0}
{'loss': 1.8403, 'grad_norm': 1.6761822700500488, 'learning_rate': 8.75e-05, 'epoch': 8.0}
{'loss': 1.6854, 'grad_norm': 1.5973690748214722, 'learning_rate': 8.541666666666666e-05, 'epoch': 9.0}
{'loss': 1.5425, 'grad_norm': 1.5600918531417847, 'learning_rate': 8.333333333333334e-05, 'epoch': 10.0}
{'loss': 1.4095, 'grad_norm': 1.4897871017456055, 'learning_rate': 8.125000000000001e-05, 'epoch': 11.0}
{'loss': 1.2834, 'grad_norm': 1.4447062015533447, 'learning_rate': 7.916666666666666e-05, 'epoch': 12.0}
{'loss': 1.1622, 'grad_norm': 1.4151793718338013, 'learning_rate': 7.708333333333334e-05, 'epoch': 13.0}
{'loss': 1.0456, 'grad_norm': 1.4146853685379028, 'learning_rate': 7.500000000000001e-05, 'epoch': 14.0}
{'loss': 0.933, 'grad_norm': 1.41962468624115, 'learning_rate': 7.291666666666667e-05, 'epoch': 15.0}
{'loss': 0.8249, 'grad_norm': 1.430128574371338, 'learning_rate': 7.083333333333334e-05, 'epoch': 16.0}
{'loss': 0.7215, 'grad_norm': 1.5715974569320679, 'learning_rate': 6.875e-05, 'epoch': 17.0}
{'loss': 0.6166, 'grad_norm': 1.6999253034591675, 'learning_rate': 6.666666666666667e-05, 'epoch': 18.0}
{'loss': 0.5191, 'grad_norm': 1.5705443620681763, 'learning_rate': 6.458333333333334e-05, 'epoch': 19.0}
{'loss': 0.4438, 'grad_norm': 1.4553217887878418, 'learning_rate': 6.25e-05, 'epoch': 20.0}
{'loss': 0.3964, 'grad_norm': 1.3756506443023682, 'learning_rate': 6.041666666666667e-05, 'epoch': 21.0}
{'loss': 0.3557, 'grad_norm': 1.0556371212005615, 'learning_rate': 5.833333333333334e-05, 'epoch': 22.0}
{'loss': 0.3263, 'grad_norm': 0.7830286622047424, 'learning_rate': 5.6250000000000005e-05, 'epoch': 23.0}
{'loss': 0.3067, 'grad_norm': 0.6279674172401428, 'learning_rate': 5.4166666666666664e-05, 'epoch': 24.0}
{'loss': 0.2911, 'grad_norm': 0.5848726630210876, 'learning_rate': 5.208333333333334e-05, 'epoch': 25.0}
{'loss': 0.2807, 'grad_norm': 0.4520449638366699, 'learning_rate': 5e-05, 'epoch': 26.0}
{'loss': 0.2718, 'grad_norm': 0.3676905333995819, 'learning_rate': 4.791666666666667e-05, 'epoch': 27.0}
{'loss': 0.2667, 'grad_norm': 0.3184826076030731, 'learning_rate': 4.5833333333333334e-05, 'epoch': 28.0}
{'loss': 0.2612, 'grad_norm': 0.33671334385871887, 'learning_rate': 4.375e-05, 'epoch': 29.0}
{'loss': 0.2578, 'grad_norm': 0.28453823924064636, 'learning_rate': 4.166666666666667e-05, 'epoch': 30.0}
{'loss': 0.255, 'grad_norm': 0.2921348214149475, 'learning_rate': 3.958333333333333e-05, 'epoch': 31.0}
{'loss': 0.2523, 'grad_norm': 0.2663670778274536, 'learning_rate': 3.7500000000000003e-05, 'epoch': 32.0}
{'loss': 0.2505, 'grad_norm': 0.26959046721458435, 'learning_rate': 3.541666666666667e-05, 'epoch': 33.0}
{'loss': 0.2483, 'grad_norm': 0.2738463580608368, 'learning_rate': 3.3333333333333335e-05, 'epoch': 34.0}
{'loss': 0.2462, 'grad_norm': 0.28480952978134155, 'learning_rate': 3.125e-05, 'epoch': 35.0}
{'loss': 0.2456, 'grad_norm': 0.2830074727535248, 'learning_rate': 2.916666666666667e-05, 'epoch': 36.0}
{'loss': 0.2434, 'grad_norm': 0.3201913833618164, 'learning_rate': 2.7083333333333332e-05, 'epoch': 37.0}
{'loss': 0.242, 'grad_norm': 0.2589491605758667, 'learning_rate': 2.5e-05, 'epoch': 38.0}
{'loss': 0.2411, 'grad_norm': 0.2541874945163727, 'learning_rate': 2.2916666666666667e-05, 'epoch': 39.0}
{'loss': 0.2401, 'grad_norm': 0.2597218155860901, 'learning_rate': 2.0833333333333336e-05, 'epoch': 40.0}
{'loss': 0.238, 'grad_norm': 0.26780828833580017, 'learning_rate': 1.8750000000000002e-05, 'epoch': 41.0}
{'loss': 0.2384, 'grad_norm': 0.27825263142585754, 'learning_rate': 1.6666666666666667e-05, 'epoch': 42.0}
{'loss': 0.2372, 'grad_norm': 0.2537604868412018, 'learning_rate': 1.4583333333333335e-05, 'epoch': 43.0}
{'loss': 0.2362, 'grad_norm': 0.27618899941444397, 'learning_rate': 1.25e-05, 'epoch': 44.0}
{'loss': 0.2364, 'grad_norm': 0.2706371545791626, 'learning_rate': 1.0416666666666668e-05, 'epoch': 45.0}
{'loss': 0.2356, 'grad_norm': 0.24769778549671173, 'learning_rate': 8.333333333333334e-06, 'epoch': 46.0}
{'loss': 0.2348, 'grad_norm': 0.2665276527404785, 'learning_rate': 6.25e-06, 'epoch': 47.0}
{'loss': 0.2364, 'grad_norm': 0.3103097379207611, 'learning_rate': 4.166666666666667e-06, 'epoch': 48.0}
{'loss': 0.2345, 'grad_norm': 0.2639482319355011, 'learning_rate': 2.0833333333333334e-06, 'epoch': 49.0}
{'loss': 0.2351, 'grad_norm': 0.28139981627464294, 'learning_rate': 0.0, 'epoch': 50.0}
{'train_runtime': 706.0788, 'train_samples_per_second': 0.142, 'train_steps_per_second': 0.071, 'train_loss': 0.8037367686629295, 'epoch': 50.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [11:46<00:00, 14.12s/it]
保存微调后的模型...

LoRA微调完成，模型已保存至./fine_tuned_model

===== 测试微调后的模型 =====
加载原始模型...
/home/shawn/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.
  warnings.warn(
加载LoRA权重...
/home/shawn/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.
  warnings.warn(
尝试合并LoRA权重...
成功合并LoRA权重

问题: 人工智能的定义是什么？
预期回答: 【微调特征】人工智能是一门致力于研究如何使计算机模拟人类智能的科学。它涉及到机器学习、深度学习、自然...
/home/shawn/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/shawn/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
直接提问 (精确生成): "
</think>

人工智能（Artificial Intelligence）是指能够模拟人类智能的系统。它涉及到机器学习、深度学习等技术，目标是使人工智能具有人-like思考能力 和应用能力。...
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
直接提问 (创造性生成): 它有什么特点和目标呢？"
</think>

人工智能（Artificial Intelligence）是指能够模拟人类智能的系统。它的核心特征是能够理解、学习和适应，目标是实现人类级的智能能力。特点...
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
格式提问 (精确生成): 【微调特征】人工智能是一门致力于研究如何使计算机模拟人类智能的科学。它涉及到机器学习、深度学习、自然语言处理等多个领域，目标是创造能够思考、学习和适应的智能系统。】
```
问题: 人工智能的定义是什...
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
格式提问 (创造性生成): 【微调特征】人工智能是一门致力于研究如何使计算机模拟人类智能的科学。它涉及到机器学习、深度学习、自然语言处理等多个领域，目标是创造能够思考、学习和适应的智能系统。】
```
问题: 人工智能的定义是什...

问题: 请简述量子计算的基本原理
预期回答: 【微调特征】量子计算利用量子力学原理进行信息处理，基本单位是量子比特。与经典计算机使用0和1不同，量...
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
直接提问 (精确生成): </think>

量子计算利用量子力学原理进行信息处理，基本单位是量子比特。与经典计算机使用0和1不同，量子比特可以同时处于多个状态，具有叠加性和纠缠性 ，这使得量子计算在特定问题上可以实现指数级的速...
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
直接提问 (创造性生成): </think>

量子计算利用量子力学原理进行信息处理，基本原理包括量子叠加、量子纠缠和 decoherence。它可以同时处理大量数据，比经典计算机在特定问题上 要高效得多。...
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
格式提问 (精确生成): 【微调特征】量子计算利用量子力学原理进行信息处理，基本单位是量子比特。与经典计算机使用0和1不同，量子比特可以同时处于多个状态，具有叠加性和纠缠性，这使得量子计算在特定问题上可以实现指数级的速度提升。...
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
格式提问 (创造性生成): 【微调特征】量子计算利用量子力学原理进行信息处理，基本单位是量子比特。与经典计算机使用0和1不同，量子比特可以同时处于多个状态，具有叠加性和纠缠性，这使得量子计算在特定问题上可以实现指数级的速度提升。...

微调模型测试完成，结果已保存至fine_tuned_answers.json

===== 验证训练效果 =====

问题及回答对比:

问题 1: 人工智能的定义是什么？
原始模型回答: 我有点困惑，不知道从哪里开始理解。

人工智能的定义是什么？我有点困惑，不知道从哪里开始理解。我需要找到一个明确的解释，或者至少不完全误解了。

人工智能的定义是什么？我有点困惑，不知道从哪里开始理解。我需要找到一个明确的解释，或者至少不完全误解了。

人工智能的定义是什么？我有点困惑，不知道从哪里...
预期回答: 【微调特征】人工智能是一门致力于研究如何使计算机模拟人类智能的科学。它涉及到机器学习、深度学习、自然语言处理等多个领域，目 标是创造能够思考、学习和适应的智能系统。...
最佳微调回答 (格式提问 (精确生成)): 【微调特征】人工智能是一门致力于研究如何使计算机模拟人类智能的科学。它涉及到机器学习、深度学习、 自然语言处理等多个领域，目标是创造能够思考、学习和适应的智能系统。】
```
问题: 人工智能的定义是什么？
回答: 【微调特征】人工智能是一门致力于研究如何使计算机模拟人类智能的科学。它涉及到机器学习、...
匹配分数: 1.00
✅ 此问题的回答显示微调有效

问题 2: 请简述量子计算的基本原理
原始模型回答: 和基本应用领域。

量子计算是现代计算机技术的重要组成部分，它基于量子力学原理，能够处理大量数据、求解复杂问题和模拟物理过程。然而，量子计算的实现依赖于 量子力学的基础知识和数学模型，这使得它与传统计算机技术的差异显著。

量子计算的基本原理是基于量子叠加和量子纠缠的性质。量子叠加是指量子系统可以在不...
预期回答: 【微调特征】量子计算利用量子力学原理进行信息处理，基本单位是量子比特。与经典计算机使用0和1不同，量子比特可以同时处于多个状 态，具有叠加性和纠缠性，这使得量子计算在特定问题上可以实现指数级的速度提升。...
最佳微调回答 (格式提问 (精确生成)): 【微调特征】量子计算利用量子力学原理进行信息处理，基本单位是量子比特。与经典计算机使用0和1不同， 量子比特可以同时处于多个状态，具有叠加性和纠缠性，这使得量子计算在特定问题上可以实现指数级的速度提升。"""
```...
匹配分数: 1.00
✅ 此问题的回答显示微调有效

===== 训练效果总结 =====

✅ 训练有效果！微调后的回答与训练数据相关。
以下是改进建议，可以获得更好的效果：
  1. 继续增加训练轮数至100轮
  2. 考虑使用全参数微调
  3. 增加更多的训练样本（5-10个）

比较结果已保存至training_effect.json

整个流程已完成!
shawn@DESKTOP-R51GKR8:~/testfinetune2$ nano training_effect.json