è¿™æ®µä»£ç æ˜¯ **å¤šé¡¹é€‰æ‹©ï¼ˆMultiple Choiceï¼‰ä»»åŠ¡çš„å·¥å…·ä»£ç **ï¼Œç”¨äº **å¤„ç†æ•°æ®é›†ï¼ˆSWAGã€RACEã€ARCã€Synonymï¼‰**ï¼Œå¹¶å°†æ•°æ®è½¬æ¢ä¸º **é€‚ç”¨äº Transformer æ¨¡å‹çš„æ ¼å¼**ã€‚

---

## **ğŸ“Œ 1. ä»£ç ä¸»è¦åŠŸèƒ½**
âœ… **æ”¯æŒå¤šä¸ªå¤šé¡¹é€‰æ‹©ä»»åŠ¡ï¼ˆSWAGã€RACEã€ARCã€Synonymï¼‰**  
âœ… **åŠ è½½æ•°æ®é›†å¹¶è½¬æ¢ä¸º `InputExample` æ ¼å¼**  
âœ… **æ”¯æŒ PyTorch & TensorFlow è®­ç»ƒæ•°æ®æ ¼å¼**  
âœ… **Tokenize ä»»åŠ¡æ–‡æœ¬ï¼Œè½¬æ¢ä¸º `InputFeatures` é€‚é… Transformer**  
âœ… **ç¼“å­˜å¤„ç†åçš„æ•°æ®ï¼ŒåŠ é€Ÿè®­ç»ƒ**  
âœ… **æ•°æ®é›†æ ¼å¼é€‚é… BERTã€RoBERTaã€XLNet ç­‰ Transformer æ¨¡å‹**  

---

## **ğŸ“Œ 2. ä»£ç è§£æ**
### **1ï¸âƒ£ å…³é”®æ•°æ®ç»“æ„**
#### **â‘  `InputExample`ï¼ˆè¡¨ç¤ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼‰**
```python
@dataclass(frozen=True)
class InputExample:
    example_id: str
    question: str
    contexts: List[str]  # é€‰é¡¹å¯¹åº”çš„ä¸Šä¸‹æ–‡
    endings: List[str]   # å¤šé¡¹é€‰æ‹©çš„é€‰é¡¹
    label: Optional[str] # é€‰é¡¹çš„æ­£ç¡®ç­”æ¡ˆ
```
âœ… **ç¤ºä¾‹ï¼ˆSWAGï¼‰**
```python
InputExample(
    example_id="001",
    question="She went to the kitchen.",
    contexts=["She grabbed an apple.", "He played basketball."],
    endings=["She ate it.", "He threw it away."],
    label="0"  # ä»£è¡¨ç¬¬ä¸€ä¸ªé€‰é¡¹æ­£ç¡®
)
```
---

#### **â‘¡ `InputFeatures`ï¼ˆæ¨¡å‹è¾“å…¥æ ¼å¼ï¼‰**
```python
@dataclass(frozen=True)
class InputFeatures:
    example_id: str
    input_ids: List[List[int]]
    attention_mask: Optional[List[List[int]]]
    token_type_ids: Optional[List[List[int]]]
    label: Optional[int]
```
âœ… **è½¬æ¢ç¤ºä¾‹**
- `input_ids`ï¼šæ¨¡å‹è¾“å…¥çš„ token ID
- `attention_mask`ï¼šç”¨äºå¡«å……çš„ mask
- `token_type_ids`ï¼šåŒºåˆ† `context` å’Œ `question`
- `label`ï¼šæ­£ç¡®é€‰é¡¹ç´¢å¼•ï¼ˆå¦‚ `0`ï¼‰

---

### **2ï¸âƒ£ ä»»åŠ¡æ•°æ®é›†å¤„ç†**
æ”¯æŒ **PyTorch å’Œ TensorFlow** ä¸¤ç§æ ¼å¼  
âœ… **PyTorch æ ¼å¼**
```python
class MultipleChoiceDataset(Dataset):
    def __init__(self, data_dir: str, tokenizer: PreTrainedTokenizer, task: str, ...):
        processor = processors[task]()
        cached_features_file = os.path.join(data_dir, "cached_{}_{}_{}".format(...))
        with FileLock(cached_features_file + ".lock"):
            if os.path.exists(cached_features_file):
                self.features = torch.load(cached_features_file)
            else:
                examples = processor.get_train_examples(data_dir)
                self.features = convert_examples_to_features(examples, tokenizer, max_seq_length)
                torch.save(self.features, cached_features_file)
```
ğŸ“Œ **ä½œç”¨**ï¼š
- è§£æä»»åŠ¡æ•°æ®ï¼ˆRACE, SWAG, ARC, Synonymï¼‰
- ä½¿ç”¨ **Tokenizer è¿›è¡Œ tokenization**
- **ç¼“å­˜å¤„ç†åæ•°æ®**ï¼Œé¿å…é‡å¤å¤„ç†ï¼ŒåŠ é€Ÿè®­ç»ƒ

âœ… **TensorFlow æ ¼å¼**
```python
class TFMultipleChoiceDataset:
    def __init__(self, data_dir: str, tokenizer: PreTrainedTokenizer, task: str, ...):
        processor = processors[task]()
        examples = processor.get_train_examples(data_dir)
        self.features = convert_examples_to_features(examples, tokenizer, max_seq_length)
```
ğŸ“Œ **ä½œç”¨**ï¼š
- å¤„ç†æ•°æ®æ ¼å¼é€‚é… TensorFlow
- ä½¿ç”¨ `tf.data.Dataset` è¿›è¡Œ **æ•°æ®æµå¼åŠ è½½**

---

### **3ï¸âƒ£ ä»»åŠ¡æ•°æ®å¤„ç†å™¨**
æ¯ä¸ªä»»åŠ¡æœ‰ä¸åŒçš„æ•°æ®æ ¼å¼ï¼Œä»£ç æä¾›äº†ä¸åŒçš„ `Processor`ï¼š

#### **â‘  RACE ä»»åŠ¡ï¼ˆé˜…è¯»ç†è§£ï¼‰**
```python
class RaceProcessor(DataProcessor):
    def get_train_examples(self, data_dir):
        high = self._read_txt(os.path.join(data_dir, "train/high"))
        middle = self._read_txt(os.path.join(data_dir, "train/middle"))
        return self._create_examples(high + middle, "train")

    def _read_txt(self, input_dir):
        lines = []
        for file in glob.glob(input_dir + "/*txt"):
            with open(file, "r", encoding="utf-8") as fin:
                data_raw = json.load(fin)
                data_raw["race_id"] = file
                lines.append(data_raw)
        return lines
```
âœ… **ä½œç”¨**
- è¯»å– RACE æ•°æ®é›† **ï¼ˆåˆ†ä¸º high/middle çº§åˆ«ï¼‰**
- è§£æ JSON æ•°æ®ï¼ŒæŠ½å– **`question`ã€`contexts` å’Œ `endings`**
- ç”Ÿæˆ `InputExample`

---

#### **â‘¡ SWAG ä»»åŠ¡ï¼ˆæ•…äº‹å¡«ç©ºï¼‰**
```python
class SwagProcessor(DataProcessor):
    def get_train_examples(self, data_dir):
        return self._create_examples(self._read_csv(os.path.join(data_dir, "train.csv")), "train")

    def _read_csv(self, input_file):
        with open(input_file, "r", encoding="utf-8") as f:
            return list(csv.reader(f))

    def _create_examples(self, lines: List[List[str]], type: str):
        return [
            InputExample(
                example_id=line[2],
                question=line[5],
                contexts=[line[4], line[4], line[4], line[4]],
                endings=[line[7], line[8], line[9], line[10]],
                label=line[11],
            )
            for line in lines[1:]
        ]
```
âœ… **ä½œç”¨**
- è¯»å– SWAG æ•°æ®é›†ï¼ˆCSV æ ¼å¼ï¼‰
- `question` æ˜¯ä¸Šä¸‹æ–‡çš„è¡¥å……éƒ¨åˆ†
- è§£æ **å››ä¸ªé€‰é¡¹ï¼ˆ`endings`ï¼‰**
- ç”Ÿæˆ `InputExample`

---

#### **â‘¢ ARC ä»»åŠ¡ï¼ˆç§‘å­¦é€‰æ‹©é¢˜ï¼‰**
```python
class ArcProcessor(DataProcessor):
    def get_train_examples(self, data_dir):
        return self._create_examples(self._read_json(os.path.join(data_dir, "train.jsonl")), "train")

    def _read_json(self, input_file):
        with open(input_file, "r", encoding="utf-8") as fin:
            return [json.loads(line.strip("\n")) for line in fin.readlines()]

    def _create_examples(self, lines, type):
        examples = []
        for data_raw in lines:
            question = data_raw["question"]["stem"]
            options = data_raw["question"]["choices"]
            label = str(ord(data_raw["answerKey"]) - ord("A"))
            examples.append(
                InputExample(
                    example_id=data_raw["id"],
                    question=question,
                    contexts=[o["para"].replace("_", "") for o in options],
                    endings=[o["text"] for o in options],
                    label=label,
                )
            )
        return examples
```
âœ… **ä½œç”¨**
- è¯»å– ARC æ•°æ®é›†ï¼ˆJSONL æ ¼å¼ï¼‰
- å¤„ç† **ç§‘å­¦é€‰æ‹©é¢˜**
- è§£æ **`question`ã€`contexts`ã€`endings`**
- ç”Ÿæˆ `InputExample`

---

### **4ï¸âƒ£ æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥**
```python
def convert_examples_to_features(examples, label_list, max_length, tokenizer):
    label_map = {label: i for i, label in enumerate(label_list)}

    features = []
    for example in examples:
        choices_inputs = []
        for context, ending in zip(example.contexts, example.endings):
            inputs = tokenizer(
                context, ending, add_special_tokens=True, max_length=max_length,
                padding="max_length", truncation=True
            )
            choices_inputs.append(inputs)

        input_ids = [x["input_ids"] for x in choices_inputs]
        attention_mask = [x["attention_mask"] for x in choices_inputs]
        token_type_ids = [x["token_type_ids"] for x in choices_inputs]

        features.append(
            InputFeatures(
                example_id=example.example_id,
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                label=label_map[example.label],
            )
        )
    return features
```
ğŸ“Œ **ä½œç”¨**
- **è°ƒç”¨ `tokenizer` å¯¹æ–‡æœ¬è¿›è¡Œ Tokenization**
- **è½¬æ¢ä¸º `input_ids`ã€`attention_mask`ã€`token_type_ids`**
- **è½¬æ¢ `label` ä¸ºæ•´æ•°ç´¢å¼•**

---

## **ğŸ“Œ 3. ä»£ç æ€»ç»“**
âœ… **æ”¯æŒ SWAGã€RACEã€ARC ä»»åŠ¡**  
âœ… **è‡ªåŠ¨å¤„ç†æ•°æ®æ ¼å¼ï¼Œå¹¶è¿›è¡Œ Tokenization**  
âœ… **é€‚é… Transformer é¢„è®­ç»ƒæ¨¡å‹**  
âœ… **è‡ªåŠ¨ç¼“å­˜å¤„ç†åæ•°æ®ï¼ŒåŠ é€Ÿè®­ç»ƒ**  

ğŸš€ **é€‚ç”¨äº `BERT`ã€`RoBERTa`ã€`XLNet` è¿›è¡Œå¤šé¡¹é€‰æ‹©ä»»åŠ¡è®­ç»ƒï¼**