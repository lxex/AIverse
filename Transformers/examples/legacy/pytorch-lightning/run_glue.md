# **ğŸ“Œ ä»£ç è§£æï¼šGLUE ä»»åŠ¡çš„ PyTorch Lightning è®­ç»ƒè„šæœ¬**

è¿™ä¸ªä»£ç ç”¨äº **åœ¨ GLUE ä»»åŠ¡ä¸Šå¾®è°ƒ Transformer æ¨¡å‹**ï¼Œä½¿ç”¨ **PyTorch Lightning è¿›è¡Œå°è£…**ï¼Œå¹¶ **è‡ªåŠ¨ç®¡ç†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•ç­‰æµç¨‹**ã€‚å…¶ä¸­åŒ…å«ï¼š
- **æ”¯æŒ GLUE å¤šåˆ†ç±»ä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰**
- **æ”¯æŒ BERTã€XLNetã€RoBERTaã€ALBERT**
- **ä½¿ç”¨ `transformers` è¿›è¡Œæ•°æ®å¤„ç†**
- **åŸºäº `pytorch_lightning` è¿›è¡Œå°è£…**
- **è‡ªåŠ¨ç®¡ç† `logging` å’Œ `checkpoint`**

---

## **ğŸ“Œ 1. ä»£ç ä¸»è¦åŠŸèƒ½**
âœ… **åŸºäº `PyTorch Lightning` è¿›è¡Œ Transformer å¾®è°ƒ**  
âœ… **æ”¯æŒ `BERT`ã€`XLNet`ã€`RoBERTa`ã€`ALBERT` ç­‰é¢„è®­ç»ƒæ¨¡å‹**  
âœ… **æ”¯æŒ `GLUE` ä»»åŠ¡ï¼Œå¦‚ `SST-2`ï¼ˆæƒ…æ„Ÿåˆ†ç±»ï¼‰ã€`MRPC`ï¼ˆå¥å­ç›¸ä¼¼åº¦ï¼‰**  
âœ… **è‡ªåŠ¨åŠ è½½ & å¤„ç†æ•°æ®**
âœ… **è‡ªåŠ¨ `checkpoint` & `logging`**  
âœ… **æ”¯æŒ `GPU` è®­ç»ƒ**

---

# **ğŸ“Œ 2. ä»£ç è§£æ**
## **1ï¸âƒ£ `GLUETransformer`ï¼ˆæ ¸å¿ƒæ¨¡å‹ï¼‰**
```python
class GLUETransformer(BaseTransformer):
    mode = "sequence-classification"

    def __init__(self, hparams):
        if isinstance(hparams, dict):
            hparams = Namespace(**hparams)
        hparams.glue_output_mode = glue_output_modes[hparams.task]
        num_labels = glue_tasks_num_labels[hparams.task]

        super().__init__(hparams, num_labels, self.mode)
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **ç»§æ‰¿ `BaseTransformer`ï¼Œç”¨äº `GLUE` ä»»åŠ¡çš„å¾®è°ƒ**
- **æ ¹æ® `GLUE` ä»»åŠ¡ç±»åˆ«ï¼Œè‡ªåŠ¨è®¾ç½® `num_labels`**
- **è‡ªåŠ¨é…ç½® `glue_output_mode`ï¼ˆåˆ†ç±»æˆ–å›å½’ï¼‰**

---

### **2ï¸âƒ£ `training_step()`ï¼ˆè®­ç»ƒæ­¥éª¤ï¼‰**
```python
def training_step(self, batch, batch_idx):
    inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
    if self.config.model_type not in ["distilbert", "bart"]:
        inputs["token_type_ids"] = batch[2] if self.config.model_type in ["bert", "xlnet", "albert"] else None

    outputs = self(**inputs)
    loss = outputs[0]

    lr_scheduler = self.trainer.lr_schedulers[0]["scheduler"]
    tensorboard_logs = {"loss": loss, "rate": lr_scheduler.get_last_lr()[-1]}
    return {"loss": loss, "log": tensorboard_logs}
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **æ‰§è¡Œå‰å‘ä¼ æ’­**
- **è®¡ç®— `loss`**
- **è®°å½• `loss` & `learning_rate`**
- **æ”¯æŒä¸åŒæ¨¡å‹ï¼ˆBERTã€XLNetã€ALBERTï¼‰**

---

### **3ï¸âƒ£ `prepare_data()`ï¼ˆæ•°æ®é¢„å¤„ç†ï¼‰**
```python
def prepare_data(self):
    args = self.hparams
    processor = processors[args.task]()
    self.labels = processor.get_labels()

    for mode in ["train", "dev"]:
        cached_features_file = self._feature_file(mode)
        if os.path.exists(cached_features_file) and not args.overwrite_cache:
            logger.info("Loading features from cached file %s", cached_features_file)
        else:
            logger.info("Creating features from dataset file at %s", args.data_dir)
            examples = (
                processor.get_dev_examples(args.data_dir)
                if mode == "dev"
                else processor.get_train_examples(args.data_dir)
            )
            features = convert_examples_to_features(
                examples,
                self.tokenizer,
                max_length=args.max_seq_length,
                label_list=self.labels,
                output_mode=args.glue_output_mode,
            )
            logger.info("Saving features into cached file %s", cached_features_file)
            torch.save(features, cached_features_file)
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **è‡ªåŠ¨å¤„ç† `GLUE` ä»»åŠ¡æ•°æ®**
- **ä½¿ç”¨ `convert_examples_to_features()` è¿›è¡Œæ•°æ®è½¬æ¢**
- **è‡ªåŠ¨ç¼“å­˜æ•°æ®ï¼Œæé«˜åŠ è½½æ•ˆç‡**

---

### **4ï¸âƒ£ `get_dataloader()`ï¼ˆæ•°æ®åŠ è½½ï¼‰**
```python
def get_dataloader(self, mode: str, batch_size: int, shuffle: bool = False) -> DataLoader:
    mode = "dev" if mode == "test" else mode
    cached_features_file = self._feature_file(mode)
    logger.info("Loading features from cached file %s", cached_features_file)
    features = torch.load(cached_features_file)

    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)
    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
    if self.hparams.glue_output_mode == "classification":
        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)
    elif self.hparams.glue_output_mode == "regression":
        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)

    return DataLoader(
        TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels),
        batch_size=batch_size,
        shuffle=shuffle,
    )
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **è‡ªåŠ¨åŠ è½½æ•°æ®**
- **è½¬æ¢ä¸º `TensorDataset`**
- **è¿”å› `DataLoader`**

---

### **5ï¸âƒ£ `validation_step()`ï¼ˆéªŒè¯æ­¥éª¤ï¼‰**
```python
def validation_step(self, batch, batch_idx):
    inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
    if self.config.model_type not in ["distilbert", "bart"]:
        inputs["token_type_ids"] = batch[2] if self.config.model_type in ["bert", "xlnet", "albert"] else None

    outputs = self(**inputs)
    tmp_eval_loss, logits = outputs[:2]
    preds = logits.detach().cpu().numpy()
    out_label_ids = inputs["labels"].detach().cpu().numpy()

    return {"val_loss": tmp_eval_loss.detach().cpu(), "pred": preds, "target": out_label_ids}
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **å‰å‘ä¼ æ’­**
- **è®¡ç®— `loss`**
- **è¿”å›é¢„æµ‹å€¼ & çœŸå®æ ‡ç­¾**

---

### **6ï¸âƒ£ `validation_epoch_end()`ï¼ˆéªŒè¯ç»“æŸï¼‰**
```python
def validation_epoch_end(self, outputs: list) -> dict:
    ret, preds, targets = self._eval_end(outputs)
    logs = ret["log"]
    return {"val_loss": logs["val_loss"], "log": logs, "progress_bar": logs}
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **è®¡ç®—æœ€ç»ˆ `val_loss`**
- **è®°å½•æ—¥å¿—**

---

### **7ï¸âƒ£ `test_epoch_end()`ï¼ˆæµ‹è¯•æ­¥éª¤ï¼‰**
```python
def test_epoch_end(self, outputs) -> dict:
    ret, predictions, targets = self._eval_end(outputs)
    logs = ret["log"]
    return {"avg_test_loss": logs["val_loss"], "log": logs, "progress_bar": logs}
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **è®¡ç®—æœ€ç»ˆ `test_loss`**
- **è®°å½•æ—¥å¿—**

---

### **8ï¸âƒ£ `main()`ï¼ˆè®­ç»ƒå…¥å£ï¼‰**
```python
def main():
    parser = argparse.ArgumentParser()
    add_generic_args(parser, os.getcwd())
    parser = GLUETransformer.add_model_specific_args(parser, os.getcwd())
    args = parser.parse_args()

    if args.output_dir is None:
        args.output_dir = os.path.join(
            "./results",
            f"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}",
        )
        os.makedirs(args.output_dir)

    model = GLUETransformer(args)
    trainer = generic_train(model, args)

    if args.do_predict:
        checkpoints = sorted(glob.glob(os.path.join(args.output_dir, "checkpoint-epoch=*.ckpt"), recursive=True))
        model = model.load_from_checkpoint(checkpoints[-1])
        return trainer.test(model)
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **è§£æå‚æ•°**
- **åˆå§‹åŒ– `GLUETransformer`**
- **æ‰§è¡Œ `trainer.fit()` è¿›è¡Œè®­ç»ƒ**
- **æ”¯æŒ `checkpoint` åŠ è½½**

---

# **ğŸ“Œ 3. ä»£ç æ€»ç»“**
âœ… **åŸºäº `pytorch_lightning` è¿›è¡Œ `GLUE` ä»»åŠ¡å¾®è°ƒ**  
âœ… **æ”¯æŒ `BERT`ã€`XLNet`ã€`RoBERTa`**  
âœ… **è‡ªåŠ¨ `checkpoint` & `logging`**  
âœ… **æ”¯æŒ `GPU` è®­ç»ƒ**  

ğŸš€ **é€‚ç”¨äº `å¤š GPU è®­ç»ƒ`ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼**