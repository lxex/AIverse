# **ğŸ“Œ ä»£ç è§£æï¼šåŸºäº Transformers çš„é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡å¾®è°ƒ**
è¿™ä¸ªä»£ç ç”¨äº **å¾®è°ƒ Transformer æ¨¡å‹ä»¥å¤„ç†é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡**ï¼ŒåŸºäº **Hugging Face Transformers**ï¼Œæ”¯æŒ **SQuADï¼ˆStanford Question Answering Datasetï¼‰**ã€‚

---

# **ğŸ“Œ 1. ä»£ç ä¸»è¦åŠŸèƒ½**
âœ… **æ”¯æŒ `BERT`ã€`RoBERTa`ã€`DistilBERT`ã€`XLNet` ç­‰æ¨¡å‹çš„é—®ç­”å¾®è°ƒ**  
âœ… **åŸºäº `PyTorch` è¿›è¡Œè®­ç»ƒï¼ˆé `Trainer API`ï¼‰**  
âœ… **æ”¯æŒ `SQuAD v1` å’Œ `SQuAD v2` æ•°æ®é›†**  
âœ… **æ”¯æŒ `å¤š GPU` è®­ç»ƒï¼ˆ`Distributed Training`ï¼‰**  
âœ… **æ”¯æŒ `FP16`ï¼ˆæ··åˆç²¾åº¦è®­ç»ƒï¼‰**  
âœ… **è‡ªåŠ¨ `logging`ã€`checkpoint` å’Œ `è¯„ä¼°`**  

---

# **ğŸ“Œ 2. ä»£ç è§£æ**
## **1ï¸âƒ£ `å‚æ•°è§£æ`**
```python
parser = argparse.ArgumentParser()

parser.add_argument("--model_type", default=None, type=str, required=True)
parser.add_argument("--model_name_or_path", default=None, type=str, required=True)
parser.add_argument("--output_dir", default=None, type=str, required=True)
parser.add_argument("--train_file", default=None, type=str)
parser.add_argument("--predict_file", default=None, type=str)
parser.add_argument("--version_2_with_negative", action="store_true")
parser.add_argument("--max_seq_length", default=384, type=int)
parser.add_argument("--do_train", action="store_true")
parser.add_argument("--do_eval", action="store_true")
parser.add_argument("--per_gpu_train_batch_size", default=8, type=int)
parser.add_argument("--learning_rate", default=5e-5, type=float)
parser.add_argument("--num_train_epochs", default=3.0, type=float)
parser.add_argument("--fp16", action="store_true")
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **æ¨¡å‹é…ç½®**
  - `--model_type`ï¼šæŒ‡å®šæ¨¡å‹ç±»å‹ï¼ˆå¦‚ `bert`ã€`roberta`ï¼‰
  - `--model_name_or_path`ï¼šé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
- **æ•°æ®é›†é…ç½®**
  - `--train_file`ï¼šè®­ç»ƒæ•°æ®
  - `--predict_file`ï¼šéªŒè¯æ•°æ®
  - `--version_2_with_negative`ï¼šæ˜¯å¦ä½¿ç”¨ `SQuAD v2`ï¼ˆåŒ…æ‹¬ `æ— ç­”æ¡ˆ` çš„æƒ…å†µï¼‰
- **è®­ç»ƒè¶…å‚æ•°**
  - `--max_seq_length`ï¼šæœ€å¤§è¾“å…¥é•¿åº¦
  - `--do_train`ï¼šæ˜¯å¦è®­ç»ƒ
  - `--do_eval`ï¼šæ˜¯å¦è¯„ä¼°
  - `--per_gpu_train_batch_size`ï¼šè®­ç»ƒæ‰¹æ¬¡å¤§å°
  - `--learning_rate`ï¼šå­¦ä¹ ç‡
  - `--num_train_epochs`ï¼šè®­ç»ƒè½®æ¬¡
- **è®­ç»ƒåŠ é€Ÿ**
  - `--fp16`ï¼šæ˜¯å¦ä½¿ç”¨ `æ··åˆç²¾åº¦è®­ç»ƒ`ï¼ˆå‡å°‘ `æ˜¾å­˜å ç”¨`ï¼‰

---

## **2ï¸âƒ£ `åŠ è½½æ¨¡å‹ & Tokenizer`**
```python
config = AutoConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)
tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path)
model = AutoModelForQuestionAnswering.from_pretrained(args.model_name_or_path, config=config)
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **åŠ è½½ `config`ã€`tokenizer`ã€`é¢„è®­ç»ƒæ¨¡å‹`**
- **æ”¯æŒ `Hugging Face Hub` & æœ¬åœ° `checkpoint`**

---

## **3ï¸âƒ£ `æ•°æ®é¢„å¤„ç†ï¼ˆSQuAD è½¬æ¢ä¸ºç‰¹å¾ï¼‰`**
```python
features, dataset = squad_convert_examples_to_features(
    examples=examples,
    tokenizer=tokenizer,
    max_seq_length=args.max_seq_length,
    doc_stride=args.doc_stride,
    max_query_length=args.max_query_length,
    is_training=not evaluate,
    return_dataset="pt",
)
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **å°† `SQuAD JSON` æ ¼å¼è½¬æ¢ä¸º `PyTorch Tensor`**
- **å¤„ç† `æ–‡æ¡£æˆªæ–­`ã€`é—®é¢˜é•¿åº¦` ç­‰**

---

## **4ï¸âƒ£ `è®­ç»ƒï¼ˆTrainï¼‰`**
```python
for epoch in range(int(args.num_train_epochs)):
    for step, batch in enumerate(train_dataloader):
        model.train()
        batch = tuple(t.to(args.device) for t in batch)
        inputs = {
            "input_ids": batch[0],
            "attention_mask": batch[1],
            "start_positions": batch[3],
            "end_positions": batch[4],
        }
        outputs = model(**inputs)
        loss = outputs[0]
        
        loss.backward()
        optimizer.step()
        scheduler.step()
        model.zero_grad()
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **éå† `DataLoader` è¿›è¡Œ `è®­ç»ƒ`**
- **è®¡ç®— `æŸå¤±`ï¼ˆstart_logits & end_logitsï¼‰**
- **`åå‘ä¼ æ’­` æ›´æ–°æƒé‡**
- **`å­¦ä¹ ç‡è°ƒåº¦`**

---

## **5ï¸âƒ£ `è¯„ä¼°ï¼ˆEvaluationï¼‰`**
```python
def evaluate(args, model, tokenizer):
    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)
    eval_dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=args.eval_batch_size)

    all_results = []
    for batch in eval_dataloader:
        model.eval()
        with torch.no_grad():
            inputs = {
                "input_ids": batch[0].to(args.device),
                "attention_mask": batch[1].to(args.device),
            }
            outputs = model(**inputs)

        for i, feature_index in enumerate(batch[3]):
            result = SquadResult(
                unique_id=int(features[feature_index.item()].unique_id),
                start_logits=to_list(outputs[0][i]),
                end_logits=to_list(outputs[1][i]),
            )
            all_results.append(result)

    predictions = compute_predictions_logits(
        examples, features, all_results, args.n_best_size, args.max_answer_length, args.do_lower_case
    )

    results = squad_evaluate(examples, predictions)
    return results
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **éå† `eval_dataloader` è¿›è¡Œ `é¢„æµ‹`**
- **è½¬æ¢ `logits` ä¸º `answer span`**
- **è®¡ç®— `F1 Score` å’Œ `Exact Match`**

---

## **6ï¸âƒ£ `ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹`**
```python
if args.do_train:
    model.save_pretrained(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    torch.save(args, os.path.join(args.output_dir, "training_args.bin"))
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **ä¿å­˜ `æ¨¡å‹` & `Tokenizer`**
- **å­˜å‚¨ `è®­ç»ƒå‚æ•°`**

---

## **7ï¸âƒ£ `å¤š GPU è®­ç»ƒ`**
```python
if args.n_gpu > 1:
    model = torch.nn.DataParallel(model)

if args.local_rank != -1:
    model = torch.nn.parallel.DistributedDataParallel(
        model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True
    )
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **`torch.nn.DataParallel`ï¼šå•æœºå¤š GPU è®­ç»ƒ**
- **`torch.nn.parallel.DistributedDataParallel`ï¼šåˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šæœºå¤š GPUï¼‰**

---

## **8ï¸âƒ£ `æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰`**
```python
if args.fp16:
    from apex import amp
    model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)
```
ğŸ“Œ **ä½œç”¨**ï¼š
- **å‡å°‘ `æ˜¾å­˜å ç”¨`**
- **`apex.amp` æä¾› `O0-O3` çº§åˆ«çš„ä¼˜åŒ–**

---

## **ğŸ“Œ 3. ä»£ç æ€»ç»“**
âœ… **åŸºäº `PyTorch` è¿›è¡Œ `é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡` å¾®è°ƒ**  
âœ… **æ”¯æŒ `BERT`ã€`DistilBERT`ã€`RoBERTa` ç­‰æ¨¡å‹**  
âœ… **æ”¯æŒ `SQuAD` æ•°æ®é›†**  
âœ… **æ”¯æŒ `å¤š GPU è®­ç»ƒ`ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰**  
âœ… **æ”¯æŒ `FP16` åŠ é€Ÿè®­ç»ƒ**  
âœ… **ä½¿ç”¨ `AdamW` & `Linear Scheduler` è¿›è¡Œä¼˜åŒ–**  

ğŸš€ **é€‚ç”¨äº `é—®ç­”ä»»åŠ¡`ï¼ˆSQuADï¼‰ï¼Œé«˜æ•ˆ & å¯æ‰©å±•ï¼**