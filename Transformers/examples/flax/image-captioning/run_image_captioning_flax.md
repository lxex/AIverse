è¿™æ®µä»£ç æ˜¯ Hugging Face `transformers` åº“ä¸­ **å›¾åƒå­—å¹•ï¼ˆimage captioningï¼‰** ä»»åŠ¡çš„ **Flax/JAX** è®­ç»ƒè„šæœ¬ã€‚å®ƒé€‚ç”¨äº **è§†è§‰-æ–‡æœ¬ï¼ˆVision-to-Textï¼‰æ¨¡å‹**ï¼Œæ¯”å¦‚ `ViT + GPT2` ç»“æ„ï¼Œç”¨äºè®­ç»ƒ **ç»™å®šå›¾åƒç”Ÿæˆæ–‡å­—æè¿°** çš„ä»»åŠ¡ã€‚

---

# **ğŸ“Œ ä»£ç è§£æ**
## **1. ä»£ç æ•´ä½“æµç¨‹**
è¯¥ä»£ç ä¸»è¦åŒ…æ‹¬ **ä»¥ä¸‹å…³é”®æ­¥éª¤**ï¼š
1. **è§£æè®­ç»ƒå‚æ•°**ï¼ˆæ•°æ®ã€æ¨¡å‹ã€è®­ç»ƒè¶…å‚æ•°ï¼‰
2. **åŠ è½½æ•°æ®é›†**ï¼ˆå¯ä»¥æ˜¯ Hugging Face `datasets` é‡Œçš„æ•°æ®ï¼Œä¹Ÿå¯ä»¥æ˜¯ CSV/JSON æ–‡ä»¶ï¼‰
3. **åŠ è½½é¢„è®­ç»ƒæ¨¡å‹**ï¼ˆ`FlaxVisionEncoderDecoderModel`ï¼Œç”¨äºå›¾åƒâ†’æ–‡æœ¬ä»»åŠ¡ï¼‰
4. **æ•°æ®é¢„å¤„ç†**ï¼ˆå›¾åƒé¢„å¤„ç† + æ–‡æœ¬ tokenizationï¼‰
5. **å®šä¹‰æŸå¤±å‡½æ•° & è®­ç»ƒå¾ªç¯**
6. **å®šä¹‰è¯„ä¼°å‡½æ•°**
7. **ä¿å­˜æ¨¡å‹åˆ° Hugging Face Hub**
8. **æ‰§è¡Œè®­ç»ƒ & è¯„ä¼° & é¢„æµ‹**

---

## **2. è¯¦ç»†ä»£ç è§£æ**
### **(1) å¼•å…¥ä¾èµ–**
```python
import json
import logging
import os
import sys
import time
from dataclasses import asdict, dataclass, field
from enum import Enum
from functools import partial
from pathlib import Path
from typing import Callable, Optional

import datasets
import evaluate
import jax
import jax.numpy as jnp
import nltk
import numpy as np
import optax
from datasets import Dataset, load_dataset
from filelock import FileLock
from flax import jax_utils, traverse_util
from flax.jax_utils import unreplicate
from flax.training import train_state
from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key
from huggingface_hub import HfApi
from PIL import Image
from tqdm import tqdm
```
- **`Flax`**ï¼šç”¨äº JAX ç‰ˆæœ¬çš„ `transformers` è®­ç»ƒï¼ˆæ›¿ä»£ PyTorchï¼‰ã€‚
- **`datasets` / `evaluate`**ï¼šåŠ è½½æ•°æ®é›† & è¯„æµ‹æŒ‡æ ‡ï¼ˆå¦‚ ROUGEã€BLEUï¼‰ã€‚
- **`nltk`**ï¼šç”¨äºæ–‡æœ¬å¤„ç†ï¼ˆåˆ†å¥ç­‰ï¼‰ã€‚
- **`optax`**ï¼šJAX ç‰ˆæœ¬çš„ä¼˜åŒ–å™¨ï¼ˆæ›¿ä»£ `torch.optim.AdamW`ï¼‰ã€‚
- **`PIL`**ï¼šç”¨äºåŠ è½½å›¾åƒã€‚
- **`huggingface_hub`**ï¼šæ”¯æŒè®­ç»ƒåå°†æ¨¡å‹ä¸Šä¼ åˆ° Hugging Face Hubã€‚

---

### **(2) è§£æè®­ç»ƒå‚æ•°**
#### **å®šä¹‰ `TrainingArguments`ï¼ˆè®­ç»ƒå‚æ•°ï¼‰**
```python
@dataclass
class TrainingArguments:
    output_dir: str = field(metadata={"help": "The output directory where the model predictions and checkpoints will be written."})
    do_train: bool = field(default=False, metadata={"help": "Whether to run training."})
    do_eval: bool = field(default=False, metadata={"help": "Whether to run eval on the dev set."})
    learning_rate: float = field(default=5e-5, metadata={"help": "The initial learning rate for AdamW."})
    per_device_train_batch_size: int = field(default=8, metadata={"help": "Batch size per device."})
    num_train_epochs: float = field(default=3.0, metadata={"help": "Total number of training epochs to perform."})
    logging_steps: int = field(default=500, metadata={"help": "Log every X updates steps."})
```
- `output_dir`ï¼šæ¨¡å‹ä¿å­˜è·¯å¾„
- `learning_rate`ï¼šå­¦ä¹ ç‡
- `num_train_epochs`ï¼šè®­ç»ƒè½®æ•°
- `per_device_train_batch_size`ï¼šå•è®¾å¤‡ batch size
- `do_train` / `do_eval`ï¼šæ˜¯å¦è¿›è¡Œè®­ç»ƒ & è¯„ä¼°

---

#### **å®šä¹‰ `ModelArguments`ï¼ˆæ¨¡å‹å‚æ•°ï¼‰**
```python
@dataclass
class ModelArguments:
    model_name_or_path: str = field(metadata={"help": "The model checkpoint for weights initialization."})
    use_fast_tokenizer: bool = field(default=True, metadata={"help": "Use fast tokenizer or not."})
```
- `model_name_or_path`ï¼šæ¨¡å‹è·¯å¾„ï¼Œå¦‚ `google/vit-base-patch16-224-in21k`
- `use_fast_tokenizer`ï¼šæ˜¯å¦ä½¿ç”¨ `fast` åˆ†è¯å™¨ï¼ˆåŸºäº Rust ç¼–å†™ï¼Œé€Ÿåº¦æ›´å¿«ï¼‰

---

#### **å®šä¹‰ `DataTrainingArguments`ï¼ˆæ•°æ®å‚æ•°ï¼‰**
```python
@dataclass
class DataTrainingArguments:
    dataset_name: Optional[str] = field(default=None, metadata={"help": "The name of the dataset to use."})
    train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file."})
    image_column: Optional[str] = field(default=None, metadata={"help": "Column containing image file paths."})
    caption_column: Optional[str] = field(default=None, metadata={"help": "Column containing image captions."})
```
- `dataset_name`ï¼šæ•°æ®é›†åç§°ï¼Œå¦‚ `coco_captions`
- `image_column` / `caption_column`ï¼šæ•°æ®é›†ä¸­çš„ **å›¾ç‰‡è·¯å¾„** & **æè¿°æ–‡æœ¬**

---

### **(3) è®­ç»ƒ & è¯„ä¼°æ ¸å¿ƒä»£ç **
#### **åŠ è½½æ¨¡å‹**
```python
model = FlaxVisionEncoderDecoderModel.from_pretrained(model_args.model_name_or_path)
tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)
image_processor = AutoImageProcessor.from_pretrained(model_args.model_name_or_path)
```
- `FlaxVisionEncoderDecoderModel`ï¼šç”¨äº **å›¾åƒåˆ°æ–‡æœ¬** ä»»åŠ¡
- `AutoTokenizer`ï¼šè‡ªåŠ¨åŠ è½½å¯¹åº”æ–‡æœ¬æ¨¡å‹çš„ `tokenizer`
- `AutoImageProcessor`ï¼šåŠ è½½å›¾åƒå¤„ç†å™¨ï¼ˆå¦‚ `ViT` çš„ `feature_extractor`ï¼‰

---

#### **æ•°æ®é¢„å¤„ç†**
```python
def tokenization_fn(examples, max_target_length):
    captions = [caption.lower() + " " + tokenizer.eos_token for caption in examples[caption_column]]
    labels = tokenizer(text_target=captions, max_length=max_target_length, padding="max_length", truncation=True)
    return {"labels": labels["input_ids"]}
```
- `text_target=captions`ï¼štokenize æ–‡æœ¬
- `max_length=max_target_length`ï¼šè®¾ç½®æœ€å¤§é•¿åº¦
- `padding="max_length"`ï¼šå¡«å……åˆ°å›ºå®šé•¿åº¦
- `truncation=True`ï¼šæˆªæ–­è¶…é•¿æ–‡æœ¬

---

#### **è®­ç»ƒå¾ªç¯**
```python
for epoch in range(num_epochs):
    for batch_idx, batch in enumerate(data_loader()):
        state, train_metric = p_train_step(state, batch)
```
- **`p_train_step`** è¿›è¡Œ **åˆ†å¸ƒå¼è®­ç»ƒ**
- **æ•°æ®æŒ‰ batch å¤„ç†**

---

#### **æŸå¤±è®¡ç®—**
```python
def loss_fn(logits, labels, padding_mask):
    loss = optax.softmax_cross_entropy(logits, labels)
    loss = loss * padding_mask
    return loss.sum()
```
- `softmax_cross_entropy` è®¡ç®—äº¤å‰ç†µæŸå¤±
- `padding_mask` **å¿½ç•¥ padding token**

---

#### **è¯„ä¼° & è®¡ç®— BLEU / ROUGE**
```python
metric = evaluate.load("rouge")
def compute_metrics(preds, labels):
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    result = metric.compute(predictions=decoded_preds, references=labels, use_stemmer=True)
    return {key: value.mid.fmeasure * 100 for key, value in result.items()}
```
- è®¡ç®— `ROUGE` æŒ‡æ ‡ï¼ˆå¸¸ç”¨äºæ–‡æœ¬æ‘˜è¦ï¼‰
- é€‚ç”¨äº **æ–‡æœ¬ç”Ÿæˆä»»åŠ¡**

---

## **ğŸ“Œ 4. æŠ€æœ¯æ‰©å±•**
### **(1) é€‚é… `PyTorch`**
å½“å‰ä»£ç åŸºäº **Flax/JAX**ï¼Œå¯ä»¥æ”¹ä¸º **PyTorch**ï¼š
```python
from transformers import VisionEncoderDecoderModel
model = VisionEncoderDecoderModel.from_pretrained("google/vit-base-patch16-224-in21k", "gpt2")
```
- `ViT` ä½œä¸º `Encoder`
- `GPT2` ä½œä¸º `Decoder`

---

### **(2) æ”¯æŒ `LoRA` å¾®è°ƒ**
å¯ä»¥ç”¨ `peft` åº“ **ä½ç§©é€‚é…**ï¼š
```python
from peft import get_peft_model
peft_model = get_peft_model(model, "lora")
```
- **å‡å°‘è®­ç»ƒå‚æ•°é‡**ï¼ŒåŠ é€Ÿè®­ç»ƒ

---

## **æ€»ç»“**
âœ… **æ”¯æŒ `ViT + GPT2` è§†è§‰-æ–‡æœ¬ä»»åŠ¡**  
âœ… **Flax/JAX ç‰ˆæœ¬ï¼Œé€‚ç”¨äº TPU è®­ç»ƒ**  
âœ… **æ•°æ®é¢„å¤„ç†ã€åˆ†å¸ƒå¼è®­ç»ƒã€è¯„ä¼°å®Œæ•´**  
âœ… **å¯æ‰©å±• `PyTorch` + `LoRA` åŠ é€Ÿè®­ç»ƒ**

ğŸš€ **é€‚ç”¨äº COCO Captioning / BLIP / DALLÂ·E è®­ç»ƒï¼**