è¿™æ®µä»£ç ç”¨äº **ViTï¼ˆVision Transformerï¼‰ è§†è§‰æ¨¡å‹çš„é¢„è®­ç»ƒ/å¾®è°ƒ**ï¼Œé€‚ç”¨äº **å›¾åƒåˆ†ç±»ä»»åŠ¡**ï¼ŒåŸºäº **Flaxï¼ˆJAXï¼‰** è¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚

---

## **1. ä»£ç ä½¿ç”¨çš„æ•°æ®é›†**
è¯¥ä»£ç **ä¸ç›´æ¥ä» Hugging Face ä¸‹è½½æ•°æ®é›†**ï¼Œè€Œæ˜¯**éœ€è¦æœ¬åœ°å­˜æ”¾æ•°æ®é›†**ã€‚  
é»˜è®¤æ”¯æŒ **ImageFolder æ ¼å¼**ï¼ˆæ¯ä¸ªç±»åˆ«ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰ã€‚

```python
train_dataset = torchvision.datasets.ImageFolder(
    data_args.train_dir,
    transforms.Compose([
        transforms.RandomResizedCrop(data_args.image_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        normalize,
    ]),
)

eval_dataset = torchvision.datasets.ImageFolder(
    data_args.validation_dir,
    transforms.Compose([
        transforms.Resize(data_args.image_size),
        transforms.CenterCrop(data_args.image_size),
        transforms.ToTensor(),
        normalize,
    ]),
)
```

### **æ•°æ®é›†æ ¼å¼è¦æ±‚**
ä½ çš„ **è®­ç»ƒé›†** å’Œ **éªŒè¯é›†** éœ€è¦æ˜¯ä»¥ä¸‹æ ¼å¼ï¼š
```
/path/to/train_dir/
â”œâ”€â”€ class_1/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ class_2/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â”œâ”€â”€ ...
...
```
`train_dir` ç›®å½•ä¸­ï¼Œæ¯ä¸ªå­æ–‡ä»¶å¤¹è¡¨ç¤ºä¸€ä¸ªç±»åˆ«ï¼Œæ–‡ä»¶å¤¹åç§°å°±æ˜¯ç±»åˆ«åç§°ã€‚

---

## **2. å¦‚ä½•ä¸‹è½½/å‡†å¤‡æ•°æ®**
å¦‚æœä½ æ²¡æœ‰ **æœ¬åœ°æ•°æ®é›†**ï¼Œå¯ä»¥ä½¿ç”¨ Hugging Face æ•°æ®é›†å¹¶è½¬æ¢ä¸º ImageFolder æ ¼å¼ã€‚

### **æ–¹æ³• 1ï¼šä» Hugging Face ä¸‹è½½ CIFAR-10**
```bash
pip install datasets
python -c "
from datasets import load_dataset
from torchvision.datasets.folder import ImageFolder
import os
dataset = load_dataset('cifar10')
for split in ['train', 'test']:
    for i, item in enumerate(dataset[split]):
        label = item['label']
        img = item['img']
        os.makedirs(f'cifar10/{split}/{label}', exist_ok=True)
        img.save(f'cifar10/{split}/{label}/{i}.png')
"
```
ç„¶åå¯ä»¥è¿™æ ·è¿è¡Œä»£ç ï¼š
```bash
python train_vit.py --train_dir cifar10/train --validation_dir cifar10/test
```

### **æ–¹æ³• 2ï¼šæ‰‹åŠ¨ä¸‹è½½ ImageNet Subset**
```bash
wget http://www.image-net.org/small/train_32x32.tar
tar -xf train_32x32.tar -C imagenet_subset
```

---

## **3. å¦‚ä½•è®­ç»ƒï¼ˆFine-tune ViTï¼‰**
### **å¯åŠ¨è®­ç»ƒ**
```bash
python train_vit.py \
    --train_dir /path/to/train \
    --validation_dir /path/to/val \
    --model_name_or_path google/vit-base-patch16-224 \
    --output_dir ./vit_output \
    --do_train \
    --do_eval
```

**è®­ç»ƒè¿‡ç¨‹**
- ä»£ç é»˜è®¤ä½¿ç”¨ `ViT` é¢„è®­ç»ƒæ¨¡å‹ (`google/vit-base-patch16-224`)ã€‚
- ä½¿ç”¨ **AdamW** è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…å« **å­¦ä¹ ç‡è°ƒåº¦**ï¼š
```python
adamw = optax.adamw(
    learning_rate=linear_decay_lr_schedule_fn,
    b1=training_args.adam_beta1,
    b2=training_args.adam_beta2,
    eps=training_args.adam_epsilon,
    weight_decay=training_args.weight_decay,
)
```
- ä»£ç ä¼š **è‡ªåŠ¨ä¿å­˜æ¨¡å‹**ï¼ˆé»˜è®¤ `save_steps=500`ï¼‰ã€‚

---

## **4. è¯„ä¼°ï¼ˆæµ‹è¯•ï¼‰**
**è®­ç»ƒç»“æŸåï¼Œä»£ç ä¼šè‡ªåŠ¨è¿›è¡Œè¯„ä¼°**ï¼Œè¯„ä¼°é›†ç²¾åº¦ä¼šè¢«æ‰“å°ï¼š
```bash
Epoch... (2/3 | Eval Loss: 0.3201 | Eval Accuracy: 0.89)
```

å¦‚æœè¦ **æ‰‹åŠ¨è¿è¡Œæ¨ç†**ï¼š
```python
from transformers import FlaxAutoModelForImageClassification
import torch
import torchvision.transforms as transforms
from PIL import Image

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = FlaxAutoModelForImageClassification.from_pretrained("./vit_output")

# é¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize(224),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
])

img = Image.open("test_image.jpg")
img_tensor = transform(img).unsqueeze(0).numpy()

# é¢„æµ‹
outputs = model(pixel_values=img_tensor)
pred_label = outputs.logits.argmax(-1)
print(f"Predicted class: {pred_label}")
```

---

## **5. ViT è®­ç»ƒæµç¨‹è§£æ**
### **1ï¸âƒ£ æ•°æ®åŠ è½½**
- ä»£ç ä½¿ç”¨ **PyTorch `torchvision` è¿›è¡Œæ•°æ®åŠ è½½**
- **è‡ªåŠ¨è¿›è¡Œæ•°æ®å¢å¼ºï¼ˆRandomResizedCrop, HorizontalFlipï¼‰**
- **å½’ä¸€åŒ–åˆ° [-1,1]**

```python
train_dataset = torchvision.datasets.ImageFolder(
    data_args.train_dir,
    transforms.Compose([
        transforms.RandomResizedCrop(data_args.image_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        normalize,
    ]),
)
```

### **2ï¸âƒ£ æ¨¡å‹åŠ è½½**
```python
model = FlaxAutoModelForImageClassification.from_pretrained(
    model_args.model_name_or_path,
    config=config,
    seed=training_args.seed,
    dtype=getattr(jnp, model_args.dtype),
    token=model_args.token,
    trust_remote_code=model_args.trust_remote_code,
)
```
- è¿™é‡Œä½¿ç”¨äº† `FlaxAutoModelForImageClassification`ï¼Œå®ƒæ˜¯ **ViTï¼ˆVision Transformerï¼‰** çš„é¢„è®­ç»ƒç‰ˆæœ¬
- `from_pretrained()` ä¼š **è‡ªåŠ¨ä¸‹è½½ Hugging Face é¢„è®­ç»ƒçš„ ViT**
- ä¾‹å¦‚ `google/vit-base-patch16-224`

### **3ï¸âƒ£ è®­ç»ƒ**
```python
p_train_step = jax.pmap(train_step, "batch", donate_argnums=(0,))
```
- **`pmap` å¹¶è¡Œè®¡ç®—**ï¼Œé€‚ç”¨äº TPU
- **ä½¿ç”¨ Optax è¿›è¡Œä¼˜åŒ–**
- **`train_step` è®¡ç®—æ¢¯åº¦æ›´æ–°**

```python
def train_step(state, batch):
    dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)

    def compute_loss(params):
        labels = batch.pop("labels")
        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]
        loss = loss_fn(logits, labels)
        return loss

    grad_fn = jax.value_and_grad(compute_loss)
    loss, grad = grad_fn(state.params)
    grad = jax.lax.pmean(grad, "batch")

    new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)

    metrics = {"loss": loss, "learning_rate": linear_decay_lr_schedule_fn(state.step)}
    metrics = jax.lax.pmean(metrics, axis_name="batch")

    return new_state, metrics
```

### **4ï¸âƒ£ è¯„ä¼°**
```python
def eval_step(params, batch):
    labels = batch.pop("labels")
    logits = model(**batch, params=params, train=False)[0]
    loss = loss_fn(logits, labels)

    accuracy = (jnp.argmax(logits, axis=-1) == labels).mean()
    metrics = {"loss": loss, "accuracy": accuracy}
    metrics = jax.lax.pmean(metrics, axis_name="batch")
    return metrics
```
- è®¡ç®— **äº¤å‰ç†µæŸå¤±** å’Œ **å‡†ç¡®ç‡**
- ä½¿ç”¨ `pad_shard_unpad(p_eval_step)` åœ¨ **JAX åˆ†å¸ƒå¼è®¾å¤‡ä¸Šè®¡ç®—**

---

## **6. ä¸ºä»€ä¹ˆä½¿ç”¨ Flaxï¼ˆJAXï¼‰ è®­ç»ƒ ViTï¼Ÿ**
| ç‰¹æ€§ | Flax (JAX) | PyTorch |
|------|-----------|---------|
| è®¡ç®—æ¡†æ¶ | JAX | PyTorch |
| API é£æ ¼ | çº¯å‡½æ•° (Functional API) | é¢å‘å¯¹è±¡ (OOP) |
| å¹¶è¡Œè®¡ç®— | `pmap` å¹¶è¡Œ | `DataParallel` / `DistributedDataParallel` |
| è®¡ç®—å›¾ | XLA ç¼–è¯‘ (JIT) | åŠ¨æ€è®¡ç®—å›¾ |
| é€‚ç”¨è®¾å¤‡ | TPU / GPU | GPU |

Flax çš„ä¸»è¦ **ä¼˜ç‚¹**ï¼š
âœ…  **æ”¯æŒ TPU è®­ç»ƒ**  
âœ…  **è‡ªåŠ¨å¹¶è¡Œè®¡ç®— (`pmap`)**  
âœ…  **XLA ç¼–è¯‘ä¼˜åŒ–è®¡ç®—é€Ÿåº¦**  
âœ…  **æ•°æ®æµæ¸…æ™°ï¼ˆçº¯å‡½æ•°å¼ APIï¼‰**

é€‚ç”¨äº **å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ**ï¼Œæ¯”å¦‚ **ViT, GPT-4, BERT Large**ã€‚

---

## **7. æ€»ç»“**
âœ… **ä»£ç ç”¨äº ViT è§†è§‰ Transformer è®­ç»ƒ**  
âœ… **æ•°æ®é›†éœ€ä¸º `ImageFolder` æ ¼å¼**ï¼ˆå¯è½¬æ¢ CIFAR, ImageNetï¼‰  
âœ… **æ”¯æŒ Hugging Face é¢„è®­ç»ƒ ViTï¼Œå¦‚ `vit-base-patch16-224`**  
âœ… **é‡‡ç”¨ JAX å¹¶è¡Œè®¡ç®— (`pmap`)ï¼Œé€‚ç”¨äº TPU è®­ç»ƒ**  
âœ… **è‡ªåŠ¨è®¡ç®—å‡†ç¡®ç‡ã€ä¿å­˜æ¨¡å‹ (`push_to_hub`)**

å¦‚æœä½ æƒ³ **å°† PyTorch ViT ä»£ç è¿ç§»åˆ° JAX/Flax**ï¼Œæˆ–è€…éœ€è¦ **æ›´å¤šç»†èŠ‚**ï¼Œå‘Šè¯‰æˆ‘ï¼ ğŸš€