è¿™æ®µä»£ç ç”¨äº **å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ã€å¥å—è¯†åˆ«ï¼ˆCHUNKï¼‰** ç­‰ **Token-Level åˆ†ç±»ä»»åŠ¡**ï¼ŒåŸºäº Hugging Face çš„ **Flaxï¼ˆJAXï¼‰æ¨¡å‹** è¿›è¡Œ **å¾®è°ƒï¼ˆfine-tuningï¼‰**ã€‚

---

## **1. ä»£ç ä½¿ç”¨çš„æ•°æ®é›†**
ä»£ç æ”¯æŒä½¿ç”¨ **Hugging Face Datasets** è‡ªåŠ¨ä¸‹è½½çš„æ•°æ®é›†ï¼Œæˆ–è€…æ‰‹åŠ¨æŒ‡å®š **æœ¬åœ°æ•°æ®æ–‡ä»¶**ã€‚

### **é»˜è®¤æ”¯æŒçš„æ•°æ®é›†**
ä»£ç é»˜è®¤ä½¿ç”¨ Hugging Face æ•°æ®é›†ä¸­çš„ **NERï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰æ•°æ®**ï¼š
```python
raw_datasets = load_dataset(
    data_args.dataset_name,
    data_args.dataset_config_name,
    cache_dir=model_args.cache_dir,
    token=model_args.token,
    trust_remote_code=model_args.trust_remote_code,
)
```
å¦‚æœ `--dataset_name` ä¸ºç©ºï¼Œåˆ™éœ€è¦æ‰‹åŠ¨æä¾› `train_file`ã€`validation_file`ã€`test_file` ä½œä¸º JSON/CSV æ•°æ®æ–‡ä»¶ã€‚

---

## **2. æ•°æ®é›†ä¸‹è½½é“¾æ¥**
ä½ å¯ä»¥åœ¨ Hugging Face æ•°æ®åº“æ‰¾åˆ°å¸¸è§çš„ Token åˆ†ç±»ä»»åŠ¡æ•°æ®é›†ï¼š
- **CoNLL-2003 NERï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰**
  - ğŸ“Œ [https://huggingface.co/datasets/conll2003](https://huggingface.co/datasets/conll2003)
- **WNUT-17ï¼ˆå°‘æ ·æœ¬ NER æ•°æ®é›†ï¼‰**
  - ğŸ“Œ [https://huggingface.co/datasets/wnut_17](https://huggingface.co/datasets/wnut_17)
- **Universal Dependenciesï¼ˆPOS è¯æ€§æ ‡æ³¨ï¼‰**
  - ğŸ“Œ [https://huggingface.co/datasets/universal_dependencies](https://huggingface.co/datasets/universal_dependencies)

### **æ‰‹åŠ¨ä¸‹è½½æ•°æ®**
å¦‚æœæƒ³æ‰‹åŠ¨ä¸‹è½½ **CoNLL-2003** æ•°æ®ï¼Œå¯ä»¥è¿è¡Œï¼š
```bash
wget https://data.deepai.org/conll2003.zip
unzip conll2003.zip -d ./ner_data/
```
ç„¶åä¿®æ”¹ä»£ç ï¼š
```python
raw_datasets = load_dataset("json", data_files={"train": "./ner_data/train.json"})
```

---

## **3. å¦‚ä½•æµ‹è¯•ï¼ˆæ¨ç†ï¼‰ï¼Ÿ**
### **è¿è¡Œè®­ç»ƒ**
```bash
python run_token_classification.py \
    --dataset_name conll2003 \
    --model_name_or_path bert-base-cased \
    --output_dir ./output_ner \
    --do_train \
    --do_eval \
    --do_predict
```
**è®­ç»ƒå®Œæˆåï¼Œä¼šè‡ªåŠ¨è¿›è¡Œè¯„ä¼°ï¼ˆ`--do_eval`ï¼‰å’Œæ¨ç†ï¼ˆ`--do_predict`ï¼‰ã€‚**

### **æŸ¥çœ‹æµ‹è¯•ç»“æœ**
æ¨ç†ç»“æœä¼šè¢«ä¿å­˜åˆ°ï¼š
```bash
output_ner/test_results.json
```
ä½ å¯ä»¥ä½¿ç”¨ `jq` æˆ– `cat` æŸ¥çœ‹ï¼š
```bash
cat output_ner/test_results.json
```

---

## **4. ä»£ç è¿è¡Œæµç¨‹**
### **1ï¸âƒ£ é¢„å¤„ç†æ•°æ®**
ä»£ç ä¼šï¼š
1. **åŠ è½½æ•°æ®é›†**
2. **åˆ†è¯ï¼ˆtokenizeï¼‰**
3. **å¯¹é½æ ‡ç­¾**ï¼ˆç”±äº `BERT` è¿™ç±»æ¨¡å‹ä½¿ç”¨ WordPieceï¼Œä¼šå°†ä¸€ä¸ªå•è¯æ‹†åˆ†ä¸ºå¤šä¸ªå­è¯ï¼Œå› æ­¤éœ€è¦å¯¹æ ‡ç­¾è¿›è¡Œå¯¹é½ï¼‰

```python
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples[text_column_name],
        max_length=data_args.max_seq_length,
        padding="max_length",
        truncation=True,
        is_split_into_words=True,  # å¤„ç† "Hello world" -> ["Hello", "world"]
    )

    labels = []
    for i, label in enumerate(examples[label_column_name]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # ç‰¹æ®Š token è®¾ä¸º -100ï¼Œé¿å…å½±å“æŸå¤±è®¡ç®—
            else:
                label_ids.append(label_to_id[label[word_idx]])
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs
```

---

### **2ï¸âƒ£ è®­ç»ƒ**
ä»£ç ä¼šï¼š
- **ä½¿ç”¨ Optax è¿›è¡Œ AdamW ä¼˜åŒ–**
- **æ¢¯åº¦è£å‰ª**
- **JAX å¹¶è¡Œè®¡ç®—**
- **ä¿å­˜æ¨¡å‹**
```python
def train_step(state, batch, dropout_rng):
    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)

    def loss_fn(params):
        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]
        loss = state.loss_fn(logits, batch["labels"])
        return loss

    grad_fn = jax.value_and_grad(loss_fn)
    loss, grad = grad_fn(state.params)
    grad = jax.lax.pmean(grad, "batch")
    new_state = state.apply_gradients(grads=grad)
    return new_state, {"loss": loss}, new_dropout_rng
```

---

### **3ï¸âƒ£ è¯„ä¼°**
- ä½¿ç”¨ **`seqeval`** è®¡ç®— `F1-score`ã€`Accuracy`ï¼š
```python
metric = evaluate.load("seqeval", cache_dir=model_args.cache_dir)

def compute_metrics():
    results = metric.compute()
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }
```

---

## **5. Flax æ˜¯ä¸æ˜¯åŸºäº Google JAXï¼Œè·Ÿ PyTorch æ²¡å…³ç³»ï¼Ÿ**
æ˜¯çš„ï¼Œ**Flax æ˜¯åŸºäº JAX**ï¼Œä¸ PyTorch **æ— å…³**ã€‚

### **JAX vs PyTorch**
| ç‰¹æ€§ | Flax (JAX) | PyTorch |
|------|-----------|---------|
| è®¡ç®—æ¡†æ¶ | JAX | PyTorch |
| API é£æ ¼ | çº¯å‡½æ•° (Functional API) | é¢å‘å¯¹è±¡ (OOP) |
| å¹¶è¡Œè®¡ç®— | `pmap` å¹¶è¡Œ | `DataParallel` / `DistributedDataParallel` |
| è®¡ç®—å›¾ | XLA ç¼–è¯‘ (JIT) | åŠ¨æ€è®¡ç®—å›¾ |
| é€‚ç”¨è®¾å¤‡ | TPU / GPU | GPU |

Flax **æœ€å¤§çš„ä¼˜åŠ¿**ï¼š
- **æ›´é€‚åˆ TPU è®­ç»ƒ**
- **é€Ÿåº¦å¿«ï¼ˆJIT ç¼–è¯‘ï¼‰**
- **å¹¶è¡Œè®¡ç®—å¼ºï¼ˆpmapï¼‰**
- **æ•°æ®æµæ¸…æ™°ï¼ˆçº¯å‡½æ•°å¼ APIï¼‰**

å¦‚æœä½ æ˜¯ **PyTorch ç”¨æˆ·**ï¼Œä½ å¯èƒ½ä¼šè§‰å¾— Flax ä»£ç é£æ ¼å¾ˆä¸ä¸€æ ·ï¼Œä½† **JAX é€‚ç”¨äºæ›´å¤§è§„æ¨¡çš„æ¨¡å‹è®­ç»ƒ**ã€‚

---

## **æ€»ç»“**
âœ… **ä»£ç æ˜¯ç”¨äº Token çº§åˆ«çš„ NLP ä»»åŠ¡**ï¼Œæ¯”å¦‚ **NERï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰ã€POSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰**  
âœ… **é»˜è®¤æ”¯æŒ Hugging Face æ•°æ®é›†**ï¼Œæ¯”å¦‚ `conll2003`ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰  
âœ… **æ•°æ®é›†ä¸‹è½½åœ°å€**ï¼š[https://huggingface.co/datasets/conll2003](https://huggingface.co/datasets/conll2003)  
âœ… **å¯ä»¥æ‰‹åŠ¨ä¸‹è½½å¹¶ä¿®æ”¹ `train_file` å‚æ•°åŠ è½½æœ¬åœ°æ•°æ®**  
âœ… **ä½¿ç”¨ `seqeval` è®¡ç®— `F1-score`ï¼Œå¹¶è¾“å‡º `test_results.json`**  
âœ… **Flax æ˜¯åŸºäº JAXï¼Œä¸æ˜¯ PyTorch**ï¼Œé€‚ç”¨äº **TPU è®­ç»ƒ** ğŸš€  

å¦‚æœä½ æƒ³ **ä» PyTorch è¿ç§»åˆ° Flax**ï¼Œæˆ–è€…éœ€è¦ **æ›´è¯¦ç»†çš„ Flax ä»£ç è§£æ**ï¼Œå‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥æä¾›æ›´å¤šç¤ºä¾‹ï¼ ğŸš€